import json
import boto3
import re

# Initialize S3 and Comprehend clients
s3_client = boto3.client('s3')
comprehend_client = boto3.client('comprehend', region_name='us-east-1')
bucket_name = "ai-3test"  # Replace with your S3 bucket name

def lambda_handler(event, context):
    # Initialize the Bedrock client
    try:
        client = boto3.client(service_name='bedrock-runtime', region_name='us-east-1')
    except Exception as e:
        return format_error_response(f"Failed to initialize Bedrock client: {str(e)}", 500)

    # Extract the prompt, subject, and user_id from the event payload
    try:
        body = event.get('body', '{}')
        if isinstance(body, str):
            body = json.loads(body)

        prompt = body.get('prompt')
        user_id = body.get('user_id')
        subject = body.get('subject')

        if not all([prompt, user_id, subject]):
            return format_error_response("Fields 'prompt', 'user_id', and 'subject' are required in the request body", 400)
    except json.JSONDecodeError:
        return format_error_response("Invalid JSON in request body", 400)

    # Step 1: Check if the question is related to the subject
    try:
        relevance_prompt = (
            f"Determine if the following query is related to the subject.\n\n"
            f"Subject: {subject}\n"
            f"Query: {prompt}\n\n"
            f"Respond with Yes if it is related and No if it is not.return yes for queries similar to give me more info,etc"
            f"Now analyze the provided subject and query."
        )

        relevance_model_id = "meta.llama3-8b-instruct-v1:0"
        relevance_response = invoke_bedrock_model(client, relevance_prompt, relevance_model_id)
        relevance_response_cleaned = relevance_response.strip().lower()

        # Log the AI's raw response
        print(f"Relevance Check Response: {relevance_response_cleaned}")

        if "yes" in relevance_response_cleaned:
            relevance_response_cleaned = "yes"
        elif "no" in relevance_response_cleaned:
            relevance_response_cleaned = "no"

        if relevance_response_cleaned not in ["yes", "no"]:
            return format_error_response(f"Unexpected response from relevance check: {relevance_response_cleaned}", 500)

        if relevance_response_cleaned == "no":
            return format_success_response({"result": "Question asked is not related to the subject"})
    except Exception as e:
        return format_error_response(f"Failed during relevance check: {str(e)}", 500)

    # Step 2: Retrieve conversation history from S3
    try:
        s3_object_key = f"{user_id}_history.json"
        conversation_history = get_conversation_history(s3_object_key)
    except Exception as e:
        return format_error_response(f"Failed to retrieve conversation history from S3: {str(e)}", 500)

    # Step 3: Append user input to conversation history
    conversation_history.append(f"User: {prompt}")
    if len(conversation_history) > 10:
        conversation_history = conversation_history[-10:]

    # Step 4: Extract keywords from the prompt
    try:
        keywords = extract_keywords(prompt)
    except Exception as e:
        return format_error_response(f"Failed to extract keywords: {str(e)}", 500)

    # Step 5: Generate the AI response
    try:
        dynamic_prompt = (
            f"You are an expert on the subject: {subject}.\n"
            "Respond concisely to the following user question based on the context provided below.\n"
            "Do not generate additional questions or repeat information.\n\n"
            "Conversation history (most recent input at the end):\n"
            + "\n".join(conversation_history) +
            f"\n\nUser: {prompt}\n\nAI:"
        )

        answer_model_id = "arn:aws:bedrock:us-east-1:396875691225:inference-profile/us.meta.llama3-3-70b-instruct-v1:0"
        final_response = invoke_bedrock_model(client, dynamic_prompt, answer_model_id)

        # Truncate the response to the last punctuation or before "User:"
        final_response = truncate_to_last_punctuation(final_response)

        # Update conversation history
        conversation_history.append(f"AI: {final_response}")
        update_conversation_history(s3_object_key, conversation_history)

        return format_success_response({
            "result": final_response,
            "keywords": keywords
        })
    except Exception as e:
        return format_error_response(f"Failed to generate AI response: {str(e)}", 500)

# Helper function to invoke the Bedrock model
def invoke_bedrock_model(client, prompt, model_id):
    try:
        response = client.invoke_model_with_response_stream(
            modelId=model_id,
            body=json.dumps({"prompt": prompt, "max_gen_len": 160, "temperature": 0.5, "top_p": 0.9}),
            contentType="application/json",
            accept="application/json"
        )
        final_response = ""
        for event in response['body']:
            if 'chunk' in event and 'bytes' in event['chunk']:
                decoded_chunk = event['chunk']['bytes'].decode('utf-8')
                try:
                    json_chunk = json.loads(decoded_chunk)
                    if 'generation' in json_chunk:
                        final_response += json_chunk['generation']
                except json.JSONDecodeError:
                    continue
        return re.sub(r'\s+', ' ', final_response).strip()
    except Exception as e:
        raise RuntimeError(f"Error invoking Bedrock model: {str(e)}")

# Helper function to truncate response to the last punctuation mark or before "User:"
def truncate_to_last_punctuation(response):
    """Truncate the response to the last full stop, exclamation mark, or before 'User:'."""
    user_index = response.find("User:")
    if user_index != -1:
        return response[:user_index].strip()
    
    last_punctuation_index = max(response.rfind('.'), response.rfind('!'))
    if last_punctuation_index != -1:
        return response[:last_punctuation_index + 1]
    
    return response

# Helper functions for S3 and Comprehend
def get_conversation_history(s3_object_key):
    try:
        response = s3_client.get_object(Bucket=bucket_name, Key=s3_object_key)
        return json.loads(response['Body'].read().decode('utf-8'))
    except s3_client.exceptions.NoSuchKey:
        return []

def update_conversation_history(s3_object_key, conversation_history):
    s3_client.put_object(
        Bucket=bucket_name,
        Key=s3_object_key,
        Body=json.dumps(conversation_history).encode('utf-8')
    )

def extract_keywords(prompt):
    response = comprehend_client.detect_key_phrases(Text=prompt, LanguageCode='en')
    return [phrase['Text'] for phrase in response.get('KeyPhrases', [])]

# Utility functions for standardized responses
def format_success_response(data):
    return {
        'statusCode': 200,
        'body': json.dumps(data),
        'headers': {'Content-Type': 'application/json'}
    }

def format_error_response(message, status_code):
    return {
        'statusCode': status_code,
        'body': json.dumps({"error": message, "statusCode": status_code}),
        'headers': {'Content-Type': 'application/json'}
    }
